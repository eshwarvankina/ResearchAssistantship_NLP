# -*- coding: utf-8 -*-
"""Text_Mining_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iQMpnZZGHQSvwJ9c_sap8WWVOhWprRbr

# Importing libraries
"""

!pip install wordcloud

!pip install pillow

import os


import pandas as pd
import numpy as np
import nltk as nlp
import matplotlib.pyplot as plt
import seaborn as sns


import scipy.stats as stats
import plotly.express as px


from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix

import nltk
nltk.download('stopwords')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from PIL import Image
import numpy as np
import urllib.request

# %matplotlib inline

from textblob import TextBlob
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import nltk
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
from collections import defaultdict

"""# Data loading"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Text Analysis Folder/data.csv')

df.head()

df.dtypes

df.shape

df = df.sort_values('Project Posted Date', ascending=False)

df.head()

df['Project Posted Date'] = pd.to_datetime(df['Project Posted Date'])


df['posted_year'] = df['Project Posted Date'].dt.year

df['posted_year'].unique()

"""Filtering out the data based on the required date range"""

start_date = '2012-01-01'
end_date = '2017-12-31'


df = df[(df['Project Posted Date'] >= start_date) & (df['Project Posted Date'] <= end_date)]

df['posted_year'].unique()

df['Project Donation Total Amount'].unique()

df['Project Donation Total Amount'] = df['Project Donation Total Amount'].str.replace(',', '')

df['Project Donation Total Amount'] = df['Project Donation Total Amount'].astype(float)

"""Added column identifier for a project to check whether it recieved funding or not

"""

df['funding_recieved'] = df['Project Donation Total Amount'].apply(lambda x: 1 if x > 0 else 0)

df['Project Total Price Including Optional Support'] = df['Project Total Price Including Optional Support'].str.replace(',', '')

df['Project Total Price Including Optional Support'] = df['Project Total Price Including Optional Support'].astype(float)

"""Added column to calculate the funding percentage the project recieved

"""

df['funding_recieved_pct'] = df['Project Donation Total Amount']/df['Project Total Price Including Optional Support']

"""Added column identifier for a project to check whether it recieved full funding or not

"""

df['fully funded'] = 0
for i in range(1, len(df)):
    if df.loc[i, 'funding_recieved_pct'] == 1:
        df.loc[i, 'fully funded'] = 1

df.head()

df = df.loc[:, ['Project Title', 'Project Need Statement', 'Project Subject Category','Project Resource Type','Project Grade Level', 'funding_recieved', 'funding_recieved_pct','fully funded']]

df['Complete_text'] = df['Project Title'] + ' ' + df['Project Need Statement']

df.head()

"""**TF IDF**"""

import nltk
nltk.download('punkt')

Complete_text_string = df['Complete_text'].str.cat(sep='')
print(Complete_text_string)

Complete_text_string = Complete_text_string.lower()

words = nltk.word_tokenize(Complete_text_string)

import re
import string

"""Remove punctuations, spaces, stopwords, extra characters

For stop words we updated the list to add words such as 'students', 'need','help','give','learning','reading' as they are very often repeated and might not uncover other possible insights
"""

table = str.maketrans('', '', string.punctuation + string.digits + ' ')
words = [word.translate(table) for word in words]
words = [word for word in words if word]

from nltk.corpus import stopwords
nltk.download('stopwords')

stopWordList = set(stopwords.words('english'))


new_words = ['students', 'need','help','give','learning','reading','classroom','school']
stopWordList.update(new_words)

cleaned_documents = [word for word in words if word.casefold() not in stopWordList]

len(cleaned_documents)

word_counts = {}
for word in cleaned_documents:
    if word not in word_counts:
        word_counts[word] = 0
    word_counts[word] += 1

sorted_words = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))

sorted_words

len(sorted_words)

"""Word cloud displays the most frequently repeated words in the requests made by the petitionors"""

wc = WordCloud(width=1800, height=1800, background_color='white', min_font_size=10)
wc.generate_from_frequencies(sorted_words)

# Display the generated image using imshow()
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Flesch- Kincaid Reading Level"""

!pip install textstat
!pip install spellchecker

import pandas as pd
import textstat

from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

df['flesch_kincaid'] = df['Complete_text'].apply(lambda x: textstat.flesch_kincaid_grade(x))

df.head()

"""Range

# Word count analysis

Add new columns to determine the total number of words in the request and the number of spelling mistakes in the request
"""

df['word_count'] = df['Complete_text'].apply(lambda x: len(str(x).split()))

import nltk
nltk.download('punkt')
nltk.download('words')

from nltk.tokenize import word_tokenize
from nltk.corpus import words

# Define function to count spelling mistakes in a sentence
def count_spelling_mistakes(sentence):
    mistake_count = 0
    word_list = word_tokenize(sentence.lower())
    for word in word_list:
        if word not in words.words():
            mistake_count += 1
    return mistake_count


# Apply function to each row of the dataframe and store result in new column
df['spelling_mistakes'] = df['Complete_text'].apply(count_spelling_mistakes)

# Print resulting dataframe
df.head()

from nltk.corpus import words

nltk.download('words')

word_list = words.words()

def count_spelling_mistakes(text):
    """
    Count the number of spelling mistakes in a given text.
    """
    words = nltk.word_tokenize(text.lower())
    # Count the number of misspelled words
    count = 0
    for word in words:
        if word not in word_list:
            count += 1
    return count

# Apply the function to the DataFrame column
df['spelling_mistakes'] = df['Complete_text'].apply(count_spelling_mistakes)
df.head()

df.head()

"""# Sentiment Analysis"""

nltk.download('vader_lexicon')

def categorize_sentiment(polarity):
    if polarity < -0.6:
        return "very negative"
    elif polarity < 0:
        return "negative"
    elif polarity == 0:
        return "neutral"
    elif polarity < 0.6:
        return "positive"
    else:
        return "very positive"

sia = SentimentIntensityAnalyzer()

df.head()

df['polarity'] = df['Complete_text'].apply(lambda x: sia.polarity_scores(x)['compound'])

# Categorize the sentiment polarity score into five buckets for each text
df['sentiment_bucket'] = df['polarity'].apply(categorize_sentiment)

# Print the DataFrame with the new 'sentiment_bucket' column
df.head()

df_sorted = df.sort_values('polarity', ascending=False)

top_50 = df_sorted.head(50)
bottom_50 = df_sorted.tail(50)

pd.set_option('display.max_colwidth', 500)

top_50['Complete_text']

bottom_50['Complete_text']

"""We see that help plays a important factor in the polarity score of the request

# Ngram columnization
"""

import string
import nltk
from collections import defaultdict
from nltk.util import ngrams
from nltk.corpus import stopwords

nltk.download('stopwords')

# Define the text corpus
corpus = Complete_text_string

# Tokenize the text and remove punctuations, digits, stopwords, and spaces
table = str.maketrans('', '', string.punctuation + string.digits)
words_n = nltk.word_tokenize(corpus.lower())
words_n = [word.translate(table) for word in words_n]
words_to_remove = {'students', 'need','help','give','learning','reading','classroom','school'}
words_n = [word.strip() for word in words_n if word not in stopwords.words('english') and word not in words_to_remove and word.strip() != '']

# Remove any empty strings from the list of words
words_n = list(filter(None, words_n))

# Define the number of grams you want to consider
n = 3

# Create a dictionary to store the n-grams and their frequency counts
ngram_counts = defaultdict(int)

# Create the n-grams and count their frequency
for gram in ngrams(words_n, n):
    ngram_counts[gram] += 1

ngram_counts = {gram: count for gram, count in ngram_counts.items() if count > 10}

sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

# Print the n-grams and their frequency counts
ngrams_list = [gram for gram, count in sorted_ngrams]

for gram, count in sorted_ngrams[:5000]:
    print(gram, count)

total_count = 0
n_grams_count = 0
for gram, count in sorted_ngrams:
    total_count += count
    n_grams_count += 1

avg_count = total_count / n_grams_count
print("Average count of n-grams", avg_count)

def create_ngram_row(text):
    # Tokenize the text and remove punctuations, digits, stopwords, and spaces
    words_n = nltk.word_tokenize(text.lower())
    words_n = [word.translate(table) for word in words_n]
    words_n = [word.strip() for word in words_n if word not in stopwords.words('english') and word.strip() != '']

    # Remove any empty strings from the list of words
    words_n = list(filter(None, words_n))

    # Create a set of the n-grams in the text
    text_ngrams = set(ngrams(words_n, n))

    # Create a dictionary indicating whether each n-gram is present in the text
    ngram_dict = {gram: 1 if gram in text_ngrams else 0 for gram in ngrams_list}
    return ngram_dict

# Apply the create_ngram_row function to each row of the dataframe and create new columns indicating whether each n-gram is present in the text
df = pd.concat([df, df['Complete_text'].apply(create_ngram_row).apply(pd.Series)], axis=1)

# Display the resulting dataframe
df.head()